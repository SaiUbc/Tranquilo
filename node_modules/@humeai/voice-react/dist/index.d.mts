import { CloseEvent } from 'hume/core';
import { MutableRefObject, PropsWithChildren, FC } from 'react';
import { AssistantEnd, AssistantMessage, AudioInput, AudioOutput, ChatMetadata, WebSocketError, JsonMessage, ToolCallMessage, ToolErrorMessage, ToolResponseMessage, UserInterruption, UserMessage } from 'hume/api/resources/empathicVoice';
import z from 'zod';
import { Hume } from 'hume';
import { Simplify } from 'type-fest';

type ConnectionMessage = {
    type: 'socket_connected';
    receivedAt: Date;
} | {
    type: 'socket_disconnected';
    code: CloseEvent['code'];
    reason: CloseEvent['reason'];
    receivedAt: Date;
};

type MicrophoneProps = {
    streamRef: MutableRefObject<MediaStream | null>;
    onAudioCaptured: (b: ArrayBuffer) => void;
    onStartRecording?: () => void;
    onStopRecording?: () => void;
    onError: (message: string) => void;
};
declare const useMicrophone: (props: MicrophoneProps) => {
    start: () => void;
    stop: () => void;
    mute: () => void;
    unmute: () => void;
    isMuted: boolean;
    fft: number[];
};

type WithReceivedAt<T> = T & {
    receivedAt: Date;
};
type AssistantEndMessage = WithReceivedAt<AssistantEnd>;
type AssistantTranscriptMessage = WithReceivedAt<AssistantMessage>;
type AudioMessage$1 = WithReceivedAt<AudioInput>;
type AudioOutputMessage = WithReceivedAt<AudioOutput>;
type ChatMetadataMessage = WithReceivedAt<ChatMetadata>;
type JSONErrorMessage = WithReceivedAt<WebSocketError>;
type JSONMessage = WithReceivedAt<JsonMessage>;
type ToolCall = WithReceivedAt<ToolCallMessage>;
type ToolError = WithReceivedAt<ToolErrorMessage>;
type ToolResponse = WithReceivedAt<ToolResponseMessage>;
type UserInterruptionMessage = WithReceivedAt<UserInterruption>;
type UserTranscriptMessage = WithReceivedAt<UserMessage>;
declare const TimeSliceSchema: z.ZodObject<{
    begin: z.ZodNumber;
    end: z.ZodNumber;
}, "strip", z.ZodTypeAny, {
    begin: number;
    end: number;
}, {
    begin: number;
    end: number;
}>;
type TimeSlice = z.infer<typeof TimeSliceSchema>;

declare const useSoundPlayer: (props: {
    onError: (message: string) => void;
    onPlayAudio: (id: string) => void;
    onStopAudio: (id: string) => void;
}) => {
    addToQueue: (message: AudioOutputMessage) => Promise<void>;
    fft: number[];
    initPlayer: () => void;
    isPlaying: boolean;
    isAudioMuted: boolean;
    muteAudio: () => void;
    unmuteAudio: () => void;
    stopAll: () => void;
    clearQueue: () => void;
    queueLength: number;
};

declare const AuthStrategySchema: z.ZodUnion<[z.ZodObject<{
    type: z.ZodLiteral<"apiKey">;
    value: z.ZodString;
}, "strip", z.ZodTypeAny, {
    type: "apiKey";
    value: string;
}, {
    type: "apiKey";
    value: string;
}>, z.ZodObject<{
    type: z.ZodLiteral<"accessToken">;
    value: z.ZodString;
}, "strip", z.ZodTypeAny, {
    type: "accessToken";
    value: string;
}, {
    type: "accessToken";
    value: string;
}>]>;
type AuthStrategy = z.infer<typeof AuthStrategySchema>;

type SocketConfig = {
    auth: AuthStrategy;
    hostname?: string;
} & Hume.empathicVoice.chat.Chat.ConnectArgs;
declare enum VoiceReadyState {
    IDLE = "idle",
    CONNECTING = "connecting",
    OPEN = "open",
    CLOSED = "closed"
}
type ToolCallHandler = (message: Simplify<ToolCall & {
    toolType: typeof Hume.empathicVoice.ToolType.Function;
}>, send: {
    success: (content: unknown) => Hume.empathicVoice.ToolResponseMessage;
    error: (e: {
        error: string;
        code: string;
        level: string;
        content: string;
    }) => Hume.empathicVoice.ToolErrorMessage;
}) => Promise<Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage>;
declare const useVoiceClient: (props: {
    onAudioMessage?: (message: AudioOutputMessage) => void;
    onMessage?: (message: JSONMessage) => void;
    onToolCall?: ToolCallHandler;
    onError?: (message: string, error?: Error) => void;
    onOpen?: () => void;
    onClose?: Hume.empathicVoice.chat.ChatSocket.EventHandlers['close'];
}) => {
    readyState: VoiceReadyState;
    sendSessionSettings: (sessionSettings: Hume.empathicVoice.SessionSettings) => void;
    sendAudio: (arrayBuffer: ArrayBufferLike) => void;
    connect: (config: SocketConfig) => Promise<unknown>;
    disconnect: () => void;
    sendUserInput: (text: string) => void;
    sendAssistantInput: (text: string) => void;
    sendToolMessage: (toolMessage: Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage) => void;
    sendPauseAssistantMessage: () => void;
    sendResumeAssistantMessage: () => void;
};

declare const useToolStatus: () => {
    store: Record<string, {
        call?: ToolCall | undefined;
        resolved?: ToolError | ToolResponse | undefined;
    }>;
    addToStore: (message: ToolCall | ToolResponse | ToolError) => void;
    clearStore: () => void;
};

type VoiceError = {
    type: 'socket_error';
    message: string;
    error?: Error;
} | {
    type: 'audio_error';
    message: string;
    error?: Error;
} | {
    type: 'mic_error';
    message: string;
    error?: Error;
};
type VoiceStatus = {
    value: 'disconnected' | 'connecting' | 'connected';
    reason?: never;
} | {
    value: 'error';
    reason: string;
};
type VoiceContextType = {
    connect: () => Promise<void>;
    disconnect: () => void;
    fft: number[];
    isMuted: boolean;
    isAudioMuted: boolean;
    isPlaying: boolean;
    messages: (JSONMessage | ConnectionMessage)[];
    lastVoiceMessage: AssistantTranscriptMessage | null;
    lastUserMessage: UserTranscriptMessage | null;
    clearMessages: () => void;
    mute: () => void;
    unmute: () => void;
    muteAudio: () => void;
    unmuteAudio: () => void;
    readyState: VoiceReadyState;
    sendUserInput: (text: string) => void;
    sendAssistantInput: (text: string) => void;
    sendSessionSettings: Hume.empathicVoice.chat.ChatSocket['sendSessionSettings'];
    sendToolMessage: (type: Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage) => void;
    pauseAssistant: () => void;
    resumeAssistant: () => void;
    status: VoiceStatus;
    micFft: number[];
    error: VoiceError | null;
    isAudioError: boolean;
    isError: boolean;
    isMicrophoneError: boolean;
    isSocketError: boolean;
    callDurationTimestamp: string | null;
    toolStatusStore: ReturnType<typeof useToolStatus>['store'];
    chatMetadata: ChatMetadataMessage | null;
    playerQueueLength: number;
    isPaused: boolean;
};
type VoiceProviderProps = PropsWithChildren<SocketConfig> & {
    sessionSettings?: Hume.empathicVoice.SessionSettings;
    onMessage?: (message: JSONMessage) => void;
    onError?: (err: VoiceError) => void;
    onOpen?: () => void;
    onClose?: Hume.empathicVoice.chat.ChatSocket.EventHandlers['close'];
    onToolCall?: ToolCallHandler;
    onAudioReceived?: (audioOutputMessage: AudioOutputMessage) => void;
    onAudioStart?: (clipId: string) => void;
    onAudioEnd?: (clipId: string) => void;
    onInterruption?: (message: UserTranscriptMessage | UserInterruptionMessage) => void;
    /**
     * @default true
     * @description Clear messages when the voice is disconnected.
     */
    clearMessagesOnDisconnect?: boolean;
    /**
     * @default 100
     * @description The maximum number of messages to keep in memory.
     */
    messageHistoryLimit?: number;
};
declare const useVoice: () => VoiceContextType;
declare const VoiceProvider: FC<VoiceProviderProps>;

declare class SocketUnknownMessageError extends Error {
    constructor(message?: string);
}
/**
 * @name isSocketUnknownMessageError
 * @description
 * Check if an error is a SocketUnknownMessageError.
 * @param err - The error to check.
 * @returns
 * `true` if the error is a SocketUnknownMessageError.
 * @example
 * ```ts
 * if (isSocketUnknownMessageError(err)) {
 * console.error('Unknown message type');
 * }
 * ```
 */
declare const isSocketUnknownMessageError: (err: unknown) => err is SocketUnknownMessageError;
declare class SocketFailedToParseMessageError extends Error {
    constructor(message?: string);
}
/**
 * @name isSocketFailedToParseMessageError
 * @description
 * Check if an error is a SocketFailedToParseMessageError.
 * @param err - The error to check.
 * @returns
 * `true` if the error is a SocketFailedToParseMessageError.
 * @example
 * ```ts
 * if (isSocketFailedToParseMessageError(err)) {
 * console.error('Failed to parse message from socket');
 * }
 * ```
 */
declare const isSocketFailedToParseMessageError: (err: unknown) => err is SocketFailedToParseMessageError;

declare const AudioMessageSchema: z.ZodEffects<z.ZodObject<{
    type: z.ZodLiteral<"audio">;
    data: z.ZodType<ArrayBuffer, z.ZodTypeDef, ArrayBuffer>;
}, "strip", z.ZodTypeAny, {
    data: ArrayBuffer;
    type: "audio";
}, {
    data: ArrayBuffer;
    type: "audio";
}>, {
    data: ArrayBuffer;
    type: "audio";
} & {
    receivedAt: Date;
}, {
    data: ArrayBuffer;
    type: "audio";
}>;
type AudioMessage = z.infer<typeof AudioMessageSchema>;

/**
 * @name parseMessageData
 * @description
 * Parse the data of a message from the socket.
 * @param data - The data to parse.
 * @returns
 * The parsed message data.
 * @example
 * ```ts
 * const message = await parseMessageData(data);
 * ```
 */
declare const parseMessageData: (data: unknown) => Promise<{
    success: true;
    message: Hume.empathicVoice.SubscribeEvent | AudioMessage;
} | {
    success: false;
    error: Error;
}>;
/**
 * @name parseMessageType
 * @description
 * Parse the type of a message from the socket.
 * @param event - The event to parse.
 * @returns
 * The parsed message type.
 * @example
 * ```ts
 * const message = await parseMessageType(event);
 * ```
 */
declare const parseMessageType: (event: MessageEvent) => Promise<{
    success: true;
    message: Hume.empathicVoice.SubscribeEvent | AudioMessage;
} | {
    success: false;
    error: Error;
}>;

declare enum Channels {
    /** Mono */
    MONO = 1,
    /** Stereo */
    STEREO = 2
}
declare enum AudioEncoding {
    /** 16-bit signed little-endian (PCM) */
    LINEAR16 = "linear16",
    /** Ogg Opus */
    OPUS = "opus"
}

declare enum LanguageModelOption {
    CLAUDE_3_OPUS = "CLAUDE_3_OPUS",
    CLAUDE_3_SONNET = "CLAUDE_3_SONNET",
    CLAUDE_3_HAIKU = "CLAUDE_3_HAIKU",
    CLAUDE_21 = "CLAUDE_21",
    CLAUDE_INSTANT_12 = "CLAUDE_INSTANT_12",
    GPT_4_TURBO_PREVIEW = "GPT_4_TURBO_PREVIEW",
    GPT_35_TURBO_0125 = "GPT_35_TURBO_0125",
    GPT_35_TURBO = "GPT_35_TURBO",
    FIREWORKS_MIXTRAL_8X7B = "FIREWORKS_MIXTRAL_8X7B"
}

declare enum TTSService {
    /** Hume's Text-To-Speech */
    DEFAULT = "hume_ai",
    /** ElevenLab's Text-To-Speech */
    ELEVEN_LABS = "eleven_labs",
    /** Play HT's Text-To-Speech */
    PLAY_HT = "play_ht"
}

export { type AssistantEndMessage, type AssistantTranscriptMessage, AudioEncoding, type AudioMessage$1 as AudioMessage, type AudioOutputMessage, Channels, type ChatMetadataMessage, type ConnectionMessage, type JSONErrorMessage, type JSONMessage, LanguageModelOption, type MicrophoneProps, type SocketConfig, SocketFailedToParseMessageError, SocketUnknownMessageError, TTSService, type TimeSlice, TimeSliceSchema, type ToolCall, type ToolCallHandler, type ToolError, type ToolResponse, type UserInterruptionMessage, type UserTranscriptMessage, type VoiceContextType, VoiceProvider, type VoiceProviderProps, VoiceReadyState, isSocketFailedToParseMessageError, isSocketUnknownMessageError, parseMessageData, parseMessageType, useMicrophone, useSoundPlayer, useVoice, useVoiceClient };
