'use client';

// src/lib/useMicrophone.ts
import { getBrowserSupportedMimeType } from "hume";
import Meyda from "meyda";
import { useCallback, useEffect, useRef, useState } from "react";

// src/lib/generateEmptyFft.ts
function generateEmptyFft() {
  return Array.from({ length: 24 }).map(() => 0);
}

// src/lib/useMicrophone.ts
var useMicrophone = (props) => {
  const { streamRef, onAudioCaptured, onError } = props;
  const [isMuted, setIsMuted] = useState(false);
  const isMutedRef = useRef(isMuted);
  const [fft, setFft] = useState(generateEmptyFft());
  const currentAnalyzer = useRef(null);
  const mimeTypeRef = useRef(null);
  const audioContext = useRef(null);
  const recorder = useRef(null);
  const sendAudio = useRef(onAudioCaptured);
  sendAudio.current = onAudioCaptured;
  const dataHandler = useCallback((event) => {
    const blob = event.data;
    blob.arrayBuffer().then((buffer) => {
      if (buffer.byteLength > 0) {
        sendAudio.current?.(buffer);
      }
    }).catch((err) => {
      console.log(err);
    });
  }, []);
  const start = useCallback(() => {
    const stream = streamRef.current;
    if (!stream) {
      throw new Error("No stream connected");
    }
    const context = new AudioContext();
    audioContext.current = context;
    const input = context.createMediaStreamSource(stream);
    try {
      currentAnalyzer.current = Meyda.createMeydaAnalyzer({
        audioContext: context,
        source: input,
        featureExtractors: ["loudness"],
        callback: (features) => {
          const newFft = features.loudness.specific || [];
          setFft(() => Array.from(newFft));
        }
      });
      currentAnalyzer.current.start();
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      console.error(`Failed to start mic analyzer: ${message}`);
    }
    const mimeType = mimeTypeRef.current;
    if (!mimeType) {
      throw new Error("No MimeType specified");
    }
    recorder.current = new MediaRecorder(stream, {
      mimeType
    });
    recorder.current.addEventListener("dataavailable", dataHandler);
    recorder.current.start(100);
  }, [dataHandler, streamRef, mimeTypeRef]);
  const stop = useCallback(() => {
    try {
      if (currentAnalyzer.current) {
        currentAnalyzer.current.stop();
        currentAnalyzer.current = null;
      }
      if (audioContext.current) {
        void audioContext.current.close().then(() => {
          audioContext.current = null;
        }).catch(() => {
          return null;
        });
      }
      recorder.current?.stop();
      recorder.current?.removeEventListener("dataavailable", dataHandler);
      recorder.current = null;
      streamRef.current?.getTracks().forEach((track) => track.stop());
      setIsMuted(false);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      onError(`Error stopping microphone: ${message}`);
      console.log(e);
    }
  }, [dataHandler, onError, streamRef]);
  const mute = useCallback(() => {
    if (currentAnalyzer.current) {
      currentAnalyzer.current.stop();
      setFft(generateEmptyFft());
    }
    streamRef.current?.getTracks().forEach((track) => {
      track.enabled = false;
    });
    isMutedRef.current = true;
    setIsMuted(true);
  }, [streamRef]);
  const unmute = useCallback(() => {
    if (currentAnalyzer.current) {
      currentAnalyzer.current.start();
    }
    streamRef.current?.getTracks().forEach((track) => {
      track.enabled = true;
    });
    isMutedRef.current = false;
    setIsMuted(false);
  }, [streamRef]);
  useEffect(() => {
    return () => {
      try {
        recorder.current?.stop();
        recorder.current?.removeEventListener("dataavailable", dataHandler);
        if (currentAnalyzer.current) {
          currentAnalyzer.current.stop();
          currentAnalyzer.current = null;
        }
        streamRef.current?.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      } catch (e) {
        console.log(e);
      }
    };
  }, [dataHandler, streamRef]);
  useEffect(() => {
    const mimeTypeResult = getBrowserSupportedMimeType();
    if (mimeTypeResult.success) {
      mimeTypeRef.current = mimeTypeResult.mimeType;
    } else {
      onError(mimeTypeResult.error.message);
    }
  }, [onError]);
  return {
    start,
    stop,
    mute,
    unmute,
    isMuted,
    fft
  };
};

// src/lib/useSoundPlayer.ts
import { convertBase64ToBlob } from "hume";
import { useCallback as useCallback2, useRef as useRef2, useState as useState2 } from "react";

// src/lib/convertFrequencyScale.ts
var barkCenterFrequencies = [
  50,
  150,
  250,
  350,
  450,
  570,
  700,
  840,
  1e3,
  1170,
  1370,
  1600,
  1850,
  2150,
  2500,
  2900,
  3400,
  4e3,
  4800,
  5800,
  7e3,
  8500,
  10500,
  13500
];
var minValue = 0;
var maxValue = 255;
function convertLinearFrequenciesToBark(linearData, sampleRate) {
  const maxFrequency = sampleRate / 2;
  const frequencyResolution = maxFrequency / linearData.length;
  const barkFrequencies = barkCenterFrequencies.map((barkFreq) => {
    const linearDataIndex = Math.round(barkFreq / frequencyResolution);
    if (linearDataIndex >= 0 && linearDataIndex < linearData.length) {
      return ((linearData[linearDataIndex] ?? 0) - minValue) / (maxValue - minValue) * 2;
    } else {
      return 0;
    }
  });
  return barkFrequencies;
}

// src/lib/useSoundPlayer.ts
var useSoundPlayer = (props) => {
  const [isPlaying, setIsPlaying] = useState2(false);
  const [isAudioMuted, setIsAudioMuted] = useState2(false);
  const [fft, setFft] = useState2(generateEmptyFft());
  const audioContext = useRef2(null);
  const analyserNode = useRef2(null);
  const gainNode = useRef2(null);
  const isInitialized = useRef2(false);
  const clipQueue = useRef2([]);
  const [queueLength, setQueueLength] = useState2(0);
  const isProcessing = useRef2(false);
  const currentlyPlayingAudioBuffer = useRef2(
    null
  );
  const frequencyDataIntervalId = useRef2(null);
  const onPlayAudio = useRef2(props.onPlayAudio);
  onPlayAudio.current = props.onPlayAudio;
  const onStopAudio = useRef2(props.onStopAudio);
  onStopAudio.current = props.onStopAudio;
  const onError = useRef2(props.onError);
  onError.current = props.onError;
  const playNextClip = useCallback2(() => {
    if (analyserNode.current === null || audioContext.current === null) {
      onError.current("Audio environment is not initialized");
      return;
    }
    if (clipQueue.current.length === 0 || isProcessing.current) {
      setQueueLength(0);
      return;
    }
    const nextClip = clipQueue.current.shift();
    setQueueLength(clipQueue.current.length);
    if (!nextClip) return;
    isProcessing.current = true;
    setIsPlaying(true);
    const bufferSource = audioContext.current.createBufferSource();
    bufferSource.buffer = nextClip.buffer;
    bufferSource.connect(analyserNode.current);
    currentlyPlayingAudioBuffer.current = bufferSource;
    const updateFrequencyData = () => {
      try {
        const bufferSampleRate = bufferSource.buffer?.sampleRate;
        if (!analyserNode.current || typeof bufferSampleRate === "undefined")
          return;
        const dataArray = new Uint8Array(
          analyserNode.current.frequencyBinCount
        );
        analyserNode.current.getByteFrequencyData(dataArray);
        const barkFrequencies = convertLinearFrequenciesToBark(
          dataArray,
          bufferSampleRate
        );
        setFft(() => barkFrequencies);
      } catch (e) {
        setFft(generateEmptyFft());
      }
    };
    frequencyDataIntervalId.current = window.setInterval(
      updateFrequencyData,
      5
    );
    bufferSource.start(0);
    onPlayAudio.current(nextClip.id);
    bufferSource.onended = () => {
      if (frequencyDataIntervalId.current) {
        clearInterval(frequencyDataIntervalId.current);
      }
      setFft(generateEmptyFft());
      bufferSource.disconnect();
      isProcessing.current = false;
      setIsPlaying(false);
      onStopAudio.current(nextClip.id);
      currentlyPlayingAudioBuffer.current = null;
      playNextClip();
    };
  }, []);
  const initPlayer = useCallback2(() => {
    const initAudioContext = new AudioContext();
    audioContext.current = initAudioContext;
    const analyser = initAudioContext.createAnalyser();
    const gain = initAudioContext.createGain();
    analyser.fftSize = 2048;
    analyser.connect(gain);
    gain.connect(initAudioContext.destination);
    analyserNode.current = analyser;
    gainNode.current = gain;
    isInitialized.current = true;
  }, []);
  const addToQueue = useCallback2(
    async (message) => {
      if (!isInitialized.current || !audioContext.current) {
        onError.current("Audio player has not been initialized");
        return;
      }
      try {
        const blob = convertBase64ToBlob(message.data, "audio/mp3");
        const arrayBuffer = await blob.arrayBuffer();
        const audioBuffer = await audioContext.current.decodeAudioData(arrayBuffer);
        clipQueue.current.push({
          id: message.id,
          buffer: audioBuffer
        });
        setQueueLength(clipQueue.current.length);
        if (clipQueue.current.length === 1) {
          playNextClip();
        }
      } catch (e) {
        const eMessage = e instanceof Error ? e.message : "Unknown error";
        onError.current(`Failed to add clip to queue: ${eMessage}`);
      }
    },
    [playNextClip]
  );
  const stopAll = useCallback2(() => {
    isInitialized.current = false;
    isProcessing.current = false;
    setIsPlaying(false);
    setIsAudioMuted(false);
    if (frequencyDataIntervalId.current) {
      window.clearInterval(frequencyDataIntervalId.current);
    }
    if (currentlyPlayingAudioBuffer.current) {
      currentlyPlayingAudioBuffer.current.disconnect();
      currentlyPlayingAudioBuffer.current = null;
    }
    if (analyserNode.current) {
      analyserNode.current.disconnect();
      analyserNode.current = null;
    }
    if (audioContext.current) {
      void audioContext.current.close().then(() => {
        audioContext.current = null;
      }).catch(() => {
        return null;
      });
    }
    clipQueue.current = [];
    setQueueLength(0);
    setFft(generateEmptyFft());
  }, []);
  const clearQueue = useCallback2(() => {
    if (currentlyPlayingAudioBuffer.current) {
      currentlyPlayingAudioBuffer.current.stop();
      currentlyPlayingAudioBuffer.current = null;
    }
    clipQueue.current = [];
    setQueueLength(0);
    isProcessing.current = false;
    setIsPlaying(false);
    setFft(generateEmptyFft());
  }, []);
  const muteAudio = useCallback2(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(0, audioContext.current.currentTime);
      setIsAudioMuted(true);
    }
  }, []);
  const unmuteAudio = useCallback2(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(1, audioContext.current.currentTime);
      setIsAudioMuted(false);
    }
  }, []);
  return {
    addToQueue,
    fft,
    initPlayer,
    isPlaying,
    isAudioMuted,
    muteAudio,
    unmuteAudio,
    stopAll,
    clearQueue,
    queueLength
  };
};

// src/lib/useVoiceClient.ts
import { Hume, HumeClient } from "hume";
import { useCallback as useCallback3, useRef as useRef3, useState as useState3 } from "react";
var isNever = (_n) => {
  return;
};
var VoiceReadyState = /* @__PURE__ */ ((VoiceReadyState2) => {
  VoiceReadyState2["IDLE"] = "idle";
  VoiceReadyState2["CONNECTING"] = "connecting";
  VoiceReadyState2["OPEN"] = "open";
  VoiceReadyState2["CLOSED"] = "closed";
  return VoiceReadyState2;
})(VoiceReadyState || {});
var useVoiceClient = (props) => {
  const client = useRef3(null);
  const [readyState, setReadyState] = useState3(
    "idle" /* IDLE */
  );
  const onAudioMessage = useRef3(
    props.onAudioMessage
  );
  onAudioMessage.current = props.onAudioMessage;
  const onMessage = useRef3(props.onMessage);
  onMessage.current = props.onMessage;
  const onToolCall = useRef3(props.onToolCall);
  onToolCall.current = props.onToolCall;
  const onError = useRef3(props.onError);
  onError.current = props.onError;
  const onOpen = useRef3(props.onOpen);
  onOpen.current = props.onOpen;
  const onClose = useRef3(props.onClose);
  onClose.current = props.onClose;
  const connect = useCallback3((config) => {
    return new Promise((resolve, reject) => {
      const hume = new HumeClient(
        config.auth.type === "apiKey" ? {
          apiKey: config.auth.value,
          environment: config.hostname
        } : {
          accessToken: config.auth.value,
          environment: config.hostname
        }
      );
      client.current = hume.empathicVoice.chat.connect(config);
      client.current.on("open", () => {
        onOpen.current?.();
        setReadyState("open" /* OPEN */);
        resolve("open" /* OPEN */);
      });
      client.current.on("message", (message) => {
        if (message.type === "audio_output") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onAudioMessage.current?.(messageWithReceivedAt);
          return;
        }
        if (message.type === "assistant_message" || message.type === "user_message" || message.type === "user_interruption" || message.type === "error" || message.type === "tool_response" || message.type === "tool_error" || message.type === "chat_metadata" || message.type === "assistant_end") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onMessage.current?.(messageWithReceivedAt);
          return;
        }
        if (message.type === "tool_call") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onMessage.current?.(messageWithReceivedAt);
          if (message.toolType === Hume.empathicVoice.ToolType.Function) {
            void onToolCall.current?.(
              {
                ...messageWithReceivedAt,
                // we have to do this because even though we are using the correct
                // enum on line 30 for the type definition
                // fern exports an interface and a value using the same `ToolType`
                // identifier so the type comparisons will always fail
                toolType: "function"
              },
              {
                success: (content) => ({
                  type: "tool_response",
                  toolCallId: messageWithReceivedAt.toolCallId,
                  content: JSON.stringify(content)
                }),
                error: ({
                  error,
                  code,
                  level,
                  content
                }) => ({
                  type: "tool_error",
                  toolCallId: messageWithReceivedAt.toolCallId,
                  error,
                  code,
                  level: level !== null ? "warn" : void 0,
                  // level can only be warn
                  content
                })
              }
            ).then((response) => {
              if (response.type === "tool_response") {
                client.current?.sendToolResponseMessage(response);
              } else if (response.type === "tool_error") {
                client.current?.sendToolErrorMessage(response);
              } else {
                onError.current?.("Invalid response from tool call");
              }
            });
          }
          return;
        }
        isNever(message);
        return;
      });
      client.current.on("close", (event) => {
        onClose.current?.(event);
        setReadyState("closed" /* CLOSED */);
      });
      client.current.on("error", (e) => {
        const message = e instanceof Error ? e.message : "Unknown error";
        onError.current?.(message, e instanceof Error ? e : void 0);
        reject(e);
      });
      setReadyState("connecting" /* CONNECTING */);
    });
  }, []);
  const disconnect = useCallback3(() => {
    setReadyState("idle" /* IDLE */);
    client.current?.close();
  }, []);
  const sendSessionSettings = useCallback3(
    (sessionSettings) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendSessionSettings(sessionSettings);
    },
    [readyState]
  );
  const sendAudio = useCallback3(
    (arrayBuffer) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.socket?.send(arrayBuffer);
    },
    [readyState]
  );
  const sendUserInput = useCallback3(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendUserInput(text);
    },
    [readyState]
  );
  const sendAssistantInput = useCallback3(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendAssistantInput({
        text
      });
    },
    [readyState]
  );
  const sendToolMessage = useCallback3(
    (toolMessage) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      if (toolMessage.type === "tool_error") {
        client.current?.sendToolErrorMessage(toolMessage);
      } else {
        client.current?.sendToolResponseMessage(toolMessage);
      }
    },
    [readyState]
  );
  const sendPauseAssistantMessage = useCallback3(() => {
    if (readyState !== "open" /* OPEN */) {
      throw new Error("Socket is not open");
    }
    client.current?.pauseAssistant({});
  }, [readyState]);
  const sendResumeAssistantMessage = useCallback3(() => {
    if (readyState !== "open" /* OPEN */) {
      throw new Error("Socket is not open");
    }
    client.current?.resumeAssistant({});
  }, [readyState]);
  return {
    readyState,
    sendSessionSettings,
    sendAudio,
    connect,
    disconnect,
    sendUserInput,
    sendAssistantInput,
    sendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  };
};

// src/lib/VoiceProvider.tsx
import {
  createContext,
  useCallback as useCallback8,
  useContext,
  useEffect as useEffect3,
  useMemo,
  useRef as useRef6,
  useState as useState8
} from "react";

// src/lib/noop.ts
var noop = () => {
};

// src/lib/useCallDuration.ts
import { intervalToDuration } from "date-fns";
import { useCallback as useCallback4, useEffect as useEffect2, useRef as useRef4, useState as useState4 } from "react";
var useCallDuration = () => {
  const interval = useRef4(null);
  const startTime = useRef4(null);
  const [timestamp, setTimestamp] = useState4(null);
  const start = useCallback4(() => {
    startTime.current = Date.now();
    setTimestamp("00:00:00");
    interval.current = window.setInterval(() => {
      if (startTime.current) {
        const duration = intervalToDuration({
          start: startTime.current,
          end: Date.now()
        });
        const hours = (duration.hours ?? 0).toString().padStart(2, "0");
        const minutes = (duration.minutes ?? 0).toString().padStart(2, "0");
        const seconds = (duration.seconds ?? 0).toString().padStart(2, "0");
        setTimestamp(`${hours}:${minutes}:${seconds}`);
      }
    }, 500);
  }, []);
  const stop = useCallback4(() => {
    if (interval.current) {
      window.clearInterval(interval.current);
      interval.current = null;
    }
  }, []);
  const reset = useCallback4(() => {
    setTimestamp(null);
  }, []);
  useEffect2(() => {
    return () => {
      if (interval.current) {
        window.clearInterval(interval.current);
        interval.current = null;
      }
    };
  }, []);
  return { timestamp, start, stop, reset };
};

// src/lib/useEncoding.ts
import { checkForAudioTracks, getAudioStream } from "hume";
import { useCallback as useCallback5, useRef as useRef5, useState as useState5 } from "react";
var useEncoding = () => {
  const [permission, setPermission] = useState5("prompt");
  const streamRef = useRef5(null);
  const getStream = useCallback5(async () => {
    try {
      const stream = await getAudioStream();
      setPermission("granted");
      streamRef.current = stream;
      checkForAudioTracks(stream);
      return "granted";
    } catch (e) {
      setPermission("denied");
      return "denied";
    }
  }, []);
  return {
    streamRef,
    getStream,
    permission
  };
};

// src/lib/useMessages.ts
import { useCallback as useCallback6, useState as useState6 } from "react";

// src/utils/index.ts
var keepLastN = (n, arr) => {
  if (arr.length <= n) {
    return arr;
  }
  return arr.slice(arr.length - n);
};

// src/lib/useMessages.ts
var useMessages = ({
  sendMessageToParent,
  messageHistoryLimit
}) => {
  const [voiceMessageMap, setVoiceMessageMap] = useState6({});
  const [messages, setMessages] = useState6([]);
  const [lastVoiceMessage, setLastVoiceMessage] = useState6(null);
  const [lastUserMessage, setLastUserMessage] = useState6(null);
  const [chatMetadata, setChatMetadata] = useState6(
    null
  );
  const createConnectMessage = useCallback6(() => {
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_connected",
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const createDisconnectMessage = useCallback6((event) => {
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_disconnected",
          code: event.code,
          reason: event.reason,
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const onMessage = useCallback6(
    (message) => {
      switch (message.type) {
        case "assistant_message":
          setVoiceMessageMap((prev) => ({
            ...prev,
            [`${message.id}`]: message
          }));
          break;
        case "user_message":
          sendMessageToParent?.(message);
          if (message.interim === false) {
            setLastUserMessage(message);
            setMessages((prev) => {
              return keepLastN(messageHistoryLimit, prev.concat([message]));
            });
          }
          break;
        case "user_interruption":
        case "error":
        case "tool_call":
        case "tool_response":
        case "tool_error":
        case "assistant_end":
          sendMessageToParent?.(message);
          setMessages((prev) => {
            return keepLastN(messageHistoryLimit, prev.concat([message]));
          });
          break;
        case "chat_metadata":
          sendMessageToParent?.(message);
          setMessages((prev) => {
            return keepLastN(messageHistoryLimit, prev.concat([message]));
          });
          setChatMetadata(message);
          break;
        default:
          break;
      }
    },
    [messageHistoryLimit, sendMessageToParent]
  );
  const onPlayAudio = useCallback6(
    (id) => {
      const matchingTranscript = voiceMessageMap[id];
      if (matchingTranscript) {
        sendMessageToParent?.(matchingTranscript);
        setLastVoiceMessage(matchingTranscript);
        setMessages((prev) => {
          return keepLastN(
            messageHistoryLimit,
            prev.concat([matchingTranscript])
          );
        });
        setVoiceMessageMap((prev) => {
          const newMap = { ...prev };
          delete newMap[id];
          return newMap;
        });
      }
    },
    [voiceMessageMap, sendMessageToParent, messageHistoryLimit]
  );
  const clearMessages = useCallback6(() => {
    setMessages([]);
    setLastVoiceMessage(null);
    setLastUserMessage(null);
    setVoiceMessageMap({});
  }, []);
  return {
    createConnectMessage,
    createDisconnectMessage,
    onMessage,
    onPlayAudio,
    clearMessages,
    messages,
    lastVoiceMessage,
    lastUserMessage,
    chatMetadata
  };
};

// src/lib/useToolStatus.ts
import { useCallback as useCallback7, useState as useState7 } from "react";
var useToolStatus = () => {
  const [store, setStore] = useState7({});
  const addToStore = useCallback7(
    (message) => {
      setStore((prev) => {
        const entry = {
          ...prev[message.toolCallId]
        };
        if (message.type === "tool_call") {
          entry.call = message;
        }
        if (message.type === "tool_response" || message.type === "tool_error") {
          entry.resolved = message;
        }
        return {
          ...prev,
          [message.toolCallId]: entry
        };
      });
    },
    []
  );
  const clearStore = useCallback7(() => {
    setStore({});
  }, []);
  return {
    store,
    addToStore,
    clearStore
  };
};

// src/lib/VoiceProvider.tsx
import { jsx } from "react/jsx-runtime";
var VoiceContext = createContext(null);
var useVoice = () => {
  const ctx = useContext(VoiceContext);
  if (!ctx) {
    throw new Error("useVoice must be used within an VoiceProvider");
  }
  return ctx;
};
var VoiceProvider = ({
  children,
  clearMessagesOnDisconnect = true,
  messageHistoryLimit = 100,
  sessionSettings,
  verboseTranscription = true,
  ...props
}) => {
  const {
    timestamp: callDurationTimestamp,
    start: startTimer,
    stop: stopTimer
  } = useCallDuration();
  const [status, setStatus] = useState8({
    value: "disconnected"
  });
  const [isPaused, setIsPaused] = useState8(false);
  const [error, setError] = useState8(null);
  const isError = error !== null;
  const isMicrophoneError = error?.type === "mic_error";
  const isSocketError = error?.type === "socket_error";
  const isAudioError = error?.type === "audio_error";
  const onError = useRef6(props.onError ?? noop);
  onError.current = props.onError ?? noop;
  const onClose = useRef6(props.onClose ?? noop);
  onClose.current = props.onClose ?? noop;
  const onMessage = useRef6(props.onMessage ?? noop);
  onMessage.current = props.onMessage ?? noop;
  const onAudioReceived = useRef6(props.onAudioReceived ?? noop);
  onAudioReceived.current = props.onAudioReceived ?? noop;
  const onAudioStart = useRef6(props.onAudioStart ?? noop);
  onAudioStart.current = props.onAudioStart ?? noop;
  const onAudioEnd = useRef6(props.onAudioEnd ?? noop);
  onAudioEnd.current = props.onAudioEnd ?? noop;
  const onInterruption = useRef6(props.onInterruption ?? noop);
  onInterruption.current = props.onInterruption ?? noop;
  const toolStatus = useToolStatus();
  const messageStore = useMessages({
    sendMessageToParent: onMessage.current,
    messageHistoryLimit
  });
  const updateError = useCallback8((err) => {
    setError(err);
    if (err !== null) {
      onError.current?.(err);
    }
  }, []);
  const onClientError = useCallback8(
    (message, err) => {
      stopTimer();
      updateError({ type: "socket_error", message, error: err });
    },
    [stopTimer, updateError]
  );
  const config = props;
  const player = useSoundPlayer({
    onError: (message) => {
      updateError({ type: "audio_error", message });
    },
    onPlayAudio: (id) => {
      messageStore.onPlayAudio(id);
      onAudioStart.current(id);
    },
    onStopAudio: (id) => {
      onAudioEnd.current(id);
    }
  });
  const { streamRef, getStream, permission: micPermission } = useEncoding();
  const client = useVoiceClient({
    onAudioMessage: (message) => {
      player.addToQueue(message);
      onAudioReceived.current(message);
    },
    onMessage: useCallback8(
      (message) => {
        messageStore.onMessage(message);
        if (message.type === "user_interruption" || message.type === "user_message") {
          if (player.isPlaying) {
            onInterruption.current(message);
          }
          player.clearQueue();
        }
        if (message.type === "tool_call" || message.type === "tool_response" || message.type === "tool_error") {
          toolStatus.addToStore(message);
        }
      },
      [messageStore, player, toolStatus]
    ),
    onError: onClientError,
    onOpen: useCallback8(() => {
      startTimer();
      messageStore.createConnectMessage();
      props.onOpen?.();
    }, [messageStore, props, startTimer]),
    onClose: useCallback8(
      (event) => {
        stopTimer();
        messageStore.createDisconnectMessage(event);
        onClose.current?.(event);
      },
      [messageStore, stopTimer]
    ),
    onToolCall: props.onToolCall
  });
  const {
    sendAudio: clientSendAudio,
    sendUserInput: clientSendUserInput,
    sendAssistantInput: clientSendAssistantInput,
    sendSessionSettings: clientSendSessionSettings,
    sendToolMessage: clientSendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  } = client;
  const mic = useMicrophone({
    streamRef,
    onAudioCaptured: useCallback8(
      (arrayBuffer) => {
        try {
          clientSendAudio(arrayBuffer);
        } catch (e) {
          const message = e instanceof Error ? e.message : "Unknown error";
          updateError({ type: "socket_error", message });
        }
      },
      [clientSendAudio, updateError]
    ),
    onError: useCallback8(
      (message) => {
        updateError({ type: "mic_error", message });
      },
      [updateError]
    )
  });
  const { clearQueue } = player;
  const pauseAssistant = useCallback8(() => {
    try {
      sendPauseAssistantMessage();
      setIsPaused(true);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({ type: "socket_error", message });
    }
    clearQueue();
  }, [sendPauseAssistantMessage, clearQueue, updateError]);
  const resumeAssistant = useCallback8(() => {
    try {
      sendResumeAssistantMessage();
      setIsPaused(false);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({ type: "socket_error", message });
    }
  }, [sendResumeAssistantMessage, updateError]);
  const connect = useCallback8(async () => {
    updateError(null);
    setStatus({ value: "connecting" });
    const permission = await getStream();
    if (permission === "denied") {
      const error2 = {
        type: "mic_error",
        message: "Microphone permission denied"
      };
      updateError(error2);
      return Promise.reject(error2);
    }
    try {
      await client.connect({
        ...config,
        verboseTranscription: true
      });
    } catch (e) {
      const error2 = {
        type: "socket_error",
        message: "We could not connect to the voice. Please try again."
      };
      updateError(error2);
      return Promise.reject(error2);
    }
    try {
      const [micPromise, playerPromise] = await Promise.allSettled([
        mic.start(),
        player.initPlayer()
      ]);
      if (micPromise.status === "fulfilled" && playerPromise.status === "fulfilled") {
        setStatus({ value: "connected" });
      }
    } catch (e) {
      const error2 = {
        type: "audio_error",
        message: e instanceof Error ? e.message : "We could not connect to audio. Please try again."
      };
      updateError(error2);
    }
  }, [client, config, getStream, mic, player, updateError]);
  const disconnectFromVoice = useCallback8(() => {
    if (client.readyState !== "closed" /* CLOSED */) {
      client.disconnect();
    }
    player.stopAll();
    mic.stop();
    if (clearMessagesOnDisconnect) {
      messageStore.clearMessages();
    }
    toolStatus.clearStore();
    setIsPaused(false);
  }, [
    client,
    player,
    mic,
    clearMessagesOnDisconnect,
    toolStatus,
    messageStore
  ]);
  const disconnect = useCallback8(
    (disconnectOnError) => {
      if (micPermission === "denied") {
        setStatus({ value: "error", reason: "Microphone permission denied" });
      }
      stopTimer();
      disconnectFromVoice();
      if (status.value !== "error" && !disconnectOnError) {
        setStatus({ value: "disconnected" });
      }
    },
    [micPermission, stopTimer, disconnectFromVoice, status.value]
  );
  useEffect3(() => {
    if (error !== null && status.value !== "error" && status.value !== "disconnected") {
      setStatus({ value: "error", reason: error.message });
      disconnectFromVoice();
    }
  }, [status.value, disconnect, disconnectFromVoice, error]);
  useEffect3(() => {
    return () => {
      disconnectFromVoice();
    };
  }, []);
  const sendUserInput = useCallback8(
    (text) => {
      try {
        clientSendUserInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendUserInput, updateError]
  );
  const sendAssistantInput = useCallback8(
    (text) => {
      try {
        clientSendAssistantInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendAssistantInput, updateError]
  );
  const sendSessionSettings = useCallback8(
    (sessionSettings2) => {
      try {
        clientSendSessionSettings(sessionSettings2);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendSessionSettings, updateError]
  );
  useEffect3(() => {
    if (client.readyState === "open" /* OPEN */ && sessionSettings !== void 0 && Object.keys(sessionSettings).length > 0) {
      sendSessionSettings(sessionSettings);
    }
  }, [client.readyState, sendSessionSettings, sessionSettings]);
  const sendToolMessage = useCallback8(
    (message) => {
      try {
        clientSendToolMessage(message);
      } catch (e) {
        const message2 = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message: message2 });
      }
    },
    [clientSendToolMessage, updateError]
  );
  const ctx = useMemo(
    () => ({
      connect,
      disconnect,
      fft: player.fft,
      micFft: mic.fft,
      isMuted: mic.isMuted,
      isAudioMuted: player.isAudioMuted,
      isPlaying: player.isPlaying,
      messages: messageStore.messages,
      lastVoiceMessage: messageStore.lastVoiceMessage,
      lastUserMessage: messageStore.lastUserMessage,
      clearMessages: messageStore.clearMessages,
      mute: mic.mute,
      muteAudio: player.muteAudio,
      readyState: client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      unmute: mic.unmute,
      unmuteAudio: player.unmuteAudio,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatusStore: toolStatus.store,
      chatMetadata: messageStore.chatMetadata,
      playerQueueLength: player.queueLength,
      isPaused
    }),
    [
      connect,
      disconnect,
      player.fft,
      player.isAudioMuted,
      player.isPlaying,
      player.muteAudio,
      player.unmuteAudio,
      player.queueLength,
      mic.fft,
      mic.isMuted,
      mic.mute,
      mic.unmute,
      messageStore.messages,
      messageStore.lastVoiceMessage,
      messageStore.lastUserMessage,
      messageStore.clearMessages,
      messageStore.chatMetadata,
      client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatus.store,
      isPaused
    ]
  );
  return /* @__PURE__ */ jsx(VoiceContext.Provider, { value: ctx, children });
};

// src/lib/errors.ts
var SocketUnknownMessageError = class extends Error {
  constructor(message) {
    super(`Unknown message type.${message ? " " + message : ""}`);
    this.name = "SocketUnknownMessageError";
  }
};
var isSocketUnknownMessageError = (err) => {
  return err instanceof SocketUnknownMessageError;
};
var SocketFailedToParseMessageError = class extends Error {
  constructor(message) {
    super(
      `Failed to parse message from socket.${message ? " " + message : ""}`
    );
    this.name = "SocketFailedToParseMessageError";
  }
};
var isSocketFailedToParseMessageError = (err) => {
  return err instanceof SocketFailedToParseMessageError;
};

// src/lib/messages.ts
import { SubscribeEvent } from "hume/serialization/resources/empathicVoice/index.js";

// src/lib/audio-message.ts
import z from "zod";
var AudioMessageSchema = z.object({
  type: z.literal("audio"),
  data: z.instanceof(ArrayBuffer)
}).transform((obj) => {
  return Object.assign(obj, {
    receivedAt: /* @__PURE__ */ new Date()
  });
});
var parseAudioMessage = async (blob) => {
  return blob.arrayBuffer().then((buffer) => {
    return {
      type: "audio",
      data: buffer,
      receivedAt: /* @__PURE__ */ new Date()
    };
  }).catch(() => {
    return null;
  });
};

// src/lib/messages.ts
var parseMessageData = async (data) => {
  if (data instanceof Blob) {
    const message = await parseAudioMessage(data);
    if (message) {
      return {
        success: true,
        message
      };
    } else {
      return {
        success: false,
        error: new SocketFailedToParseMessageError(
          `Received blob was unable to be converted to ArrayBuffer.`
        )
      };
    }
  }
  if (typeof data !== "string") {
    return {
      success: false,
      error: new SocketFailedToParseMessageError(
        `Expected a string but received ${typeof data}.`
      )
    };
  }
  const parseResponse = SubscribeEvent.parse(data);
  if (!parseResponse.ok) {
    return {
      success: false,
      error: new SocketUnknownMessageError(
        `Received JSON was not a known message type.`
      )
    };
  }
  return {
    success: true,
    message: parseResponse.value
  };
};
var parseMessageType = async (event) => {
  const data = event.data;
  return parseMessageData(data);
};

// src/models/audio.ts
var Channels = /* @__PURE__ */ ((Channels2) => {
  Channels2[Channels2["MONO"] = 1] = "MONO";
  Channels2[Channels2["STEREO"] = 2] = "STEREO";
  return Channels2;
})(Channels || {});
var AudioEncoding = /* @__PURE__ */ ((AudioEncoding2) => {
  AudioEncoding2["LINEAR16"] = "linear16";
  AudioEncoding2["OPUS"] = "opus";
  return AudioEncoding2;
})(AudioEncoding || {});

// src/models/llm.ts
var LanguageModelOption = /* @__PURE__ */ ((LanguageModelOption2) => {
  LanguageModelOption2["CLAUDE_3_OPUS"] = "CLAUDE_3_OPUS";
  LanguageModelOption2["CLAUDE_3_SONNET"] = "CLAUDE_3_SONNET";
  LanguageModelOption2["CLAUDE_3_HAIKU"] = "CLAUDE_3_HAIKU";
  LanguageModelOption2["CLAUDE_21"] = "CLAUDE_21";
  LanguageModelOption2["CLAUDE_INSTANT_12"] = "CLAUDE_INSTANT_12";
  LanguageModelOption2["GPT_4_TURBO_PREVIEW"] = "GPT_4_TURBO_PREVIEW";
  LanguageModelOption2["GPT_35_TURBO_0125"] = "GPT_35_TURBO_0125";
  LanguageModelOption2["GPT_35_TURBO"] = "GPT_35_TURBO";
  LanguageModelOption2["FIREWORKS_MIXTRAL_8X7B"] = "FIREWORKS_MIXTRAL_8X7B";
  return LanguageModelOption2;
})(LanguageModelOption || {});

// src/models/messages.ts
import z2 from "zod";
var TimeSliceSchema = z2.object({
  begin: z2.number(),
  end: z2.number()
});

// src/models/ttsService.ts
var TTSService = /* @__PURE__ */ ((TTSService2) => {
  TTSService2["DEFAULT"] = "hume_ai";
  TTSService2["ELEVEN_LABS"] = "eleven_labs";
  TTSService2["PLAY_HT"] = "play_ht";
  return TTSService2;
})(TTSService || {});
export {
  AudioEncoding,
  Channels,
  LanguageModelOption,
  SocketFailedToParseMessageError,
  SocketUnknownMessageError,
  TTSService,
  TimeSliceSchema,
  VoiceProvider,
  VoiceReadyState,
  isSocketFailedToParseMessageError,
  isSocketUnknownMessageError,
  parseMessageData,
  parseMessageType,
  useMicrophone,
  useSoundPlayer,
  useVoice,
  useVoiceClient
};
//# sourceMappingURL=index.mjs.map