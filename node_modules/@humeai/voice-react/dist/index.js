'use client';
"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var src_exports = {};
__export(src_exports, {
  AudioEncoding: () => AudioEncoding,
  Channels: () => Channels,
  LanguageModelOption: () => LanguageModelOption,
  SocketFailedToParseMessageError: () => SocketFailedToParseMessageError,
  SocketUnknownMessageError: () => SocketUnknownMessageError,
  TTSService: () => TTSService,
  TimeSliceSchema: () => TimeSliceSchema,
  VoiceProvider: () => VoiceProvider,
  VoiceReadyState: () => VoiceReadyState,
  isSocketFailedToParseMessageError: () => isSocketFailedToParseMessageError,
  isSocketUnknownMessageError: () => isSocketUnknownMessageError,
  parseMessageData: () => parseMessageData,
  parseMessageType: () => parseMessageType,
  useMicrophone: () => useMicrophone,
  useSoundPlayer: () => useSoundPlayer,
  useVoice: () => useVoice,
  useVoiceClient: () => useVoiceClient
});
module.exports = __toCommonJS(src_exports);

// src/lib/useMicrophone.ts
var import_hume = require("hume");
var import_meyda = __toESM(require("meyda"));
var import_react = require("react");

// src/lib/generateEmptyFft.ts
function generateEmptyFft() {
  return Array.from({ length: 24 }).map(() => 0);
}

// src/lib/useMicrophone.ts
var useMicrophone = (props) => {
  const { streamRef, onAudioCaptured, onError } = props;
  const [isMuted, setIsMuted] = (0, import_react.useState)(false);
  const isMutedRef = (0, import_react.useRef)(isMuted);
  const [fft, setFft] = (0, import_react.useState)(generateEmptyFft());
  const currentAnalyzer = (0, import_react.useRef)(null);
  const mimeTypeRef = (0, import_react.useRef)(null);
  const audioContext = (0, import_react.useRef)(null);
  const recorder = (0, import_react.useRef)(null);
  const sendAudio = (0, import_react.useRef)(onAudioCaptured);
  sendAudio.current = onAudioCaptured;
  const dataHandler = (0, import_react.useCallback)((event) => {
    const blob = event.data;
    blob.arrayBuffer().then((buffer) => {
      if (buffer.byteLength > 0) {
        sendAudio.current?.(buffer);
      }
    }).catch((err) => {
      console.log(err);
    });
  }, []);
  const start = (0, import_react.useCallback)(() => {
    const stream = streamRef.current;
    if (!stream) {
      throw new Error("No stream connected");
    }
    const context = new AudioContext();
    audioContext.current = context;
    const input = context.createMediaStreamSource(stream);
    try {
      currentAnalyzer.current = import_meyda.default.createMeydaAnalyzer({
        audioContext: context,
        source: input,
        featureExtractors: ["loudness"],
        callback: (features) => {
          const newFft = features.loudness.specific || [];
          setFft(() => Array.from(newFft));
        }
      });
      currentAnalyzer.current.start();
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      console.error(`Failed to start mic analyzer: ${message}`);
    }
    const mimeType = mimeTypeRef.current;
    if (!mimeType) {
      throw new Error("No MimeType specified");
    }
    recorder.current = new MediaRecorder(stream, {
      mimeType
    });
    recorder.current.addEventListener("dataavailable", dataHandler);
    recorder.current.start(100);
  }, [dataHandler, streamRef, mimeTypeRef]);
  const stop = (0, import_react.useCallback)(() => {
    try {
      if (currentAnalyzer.current) {
        currentAnalyzer.current.stop();
        currentAnalyzer.current = null;
      }
      if (audioContext.current) {
        void audioContext.current.close().then(() => {
          audioContext.current = null;
        }).catch(() => {
          return null;
        });
      }
      recorder.current?.stop();
      recorder.current?.removeEventListener("dataavailable", dataHandler);
      recorder.current = null;
      streamRef.current?.getTracks().forEach((track) => track.stop());
      setIsMuted(false);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      onError(`Error stopping microphone: ${message}`);
      console.log(e);
    }
  }, [dataHandler, onError, streamRef]);
  const mute = (0, import_react.useCallback)(() => {
    if (currentAnalyzer.current) {
      currentAnalyzer.current.stop();
      setFft(generateEmptyFft());
    }
    streamRef.current?.getTracks().forEach((track) => {
      track.enabled = false;
    });
    isMutedRef.current = true;
    setIsMuted(true);
  }, [streamRef]);
  const unmute = (0, import_react.useCallback)(() => {
    if (currentAnalyzer.current) {
      currentAnalyzer.current.start();
    }
    streamRef.current?.getTracks().forEach((track) => {
      track.enabled = true;
    });
    isMutedRef.current = false;
    setIsMuted(false);
  }, [streamRef]);
  (0, import_react.useEffect)(() => {
    return () => {
      try {
        recorder.current?.stop();
        recorder.current?.removeEventListener("dataavailable", dataHandler);
        if (currentAnalyzer.current) {
          currentAnalyzer.current.stop();
          currentAnalyzer.current = null;
        }
        streamRef.current?.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      } catch (e) {
        console.log(e);
      }
    };
  }, [dataHandler, streamRef]);
  (0, import_react.useEffect)(() => {
    const mimeTypeResult = (0, import_hume.getBrowserSupportedMimeType)();
    if (mimeTypeResult.success) {
      mimeTypeRef.current = mimeTypeResult.mimeType;
    } else {
      onError(mimeTypeResult.error.message);
    }
  }, [onError]);
  return {
    start,
    stop,
    mute,
    unmute,
    isMuted,
    fft
  };
};

// src/lib/useSoundPlayer.ts
var import_hume2 = require("hume");
var import_react2 = require("react");

// src/lib/convertFrequencyScale.ts
var barkCenterFrequencies = [
  50,
  150,
  250,
  350,
  450,
  570,
  700,
  840,
  1e3,
  1170,
  1370,
  1600,
  1850,
  2150,
  2500,
  2900,
  3400,
  4e3,
  4800,
  5800,
  7e3,
  8500,
  10500,
  13500
];
var minValue = 0;
var maxValue = 255;
function convertLinearFrequenciesToBark(linearData, sampleRate) {
  const maxFrequency = sampleRate / 2;
  const frequencyResolution = maxFrequency / linearData.length;
  const barkFrequencies = barkCenterFrequencies.map((barkFreq) => {
    const linearDataIndex = Math.round(barkFreq / frequencyResolution);
    if (linearDataIndex >= 0 && linearDataIndex < linearData.length) {
      return ((linearData[linearDataIndex] ?? 0) - minValue) / (maxValue - minValue) * 2;
    } else {
      return 0;
    }
  });
  return barkFrequencies;
}

// src/lib/useSoundPlayer.ts
var useSoundPlayer = (props) => {
  const [isPlaying, setIsPlaying] = (0, import_react2.useState)(false);
  const [isAudioMuted, setIsAudioMuted] = (0, import_react2.useState)(false);
  const [fft, setFft] = (0, import_react2.useState)(generateEmptyFft());
  const audioContext = (0, import_react2.useRef)(null);
  const analyserNode = (0, import_react2.useRef)(null);
  const gainNode = (0, import_react2.useRef)(null);
  const isInitialized = (0, import_react2.useRef)(false);
  const clipQueue = (0, import_react2.useRef)([]);
  const [queueLength, setQueueLength] = (0, import_react2.useState)(0);
  const isProcessing = (0, import_react2.useRef)(false);
  const currentlyPlayingAudioBuffer = (0, import_react2.useRef)(
    null
  );
  const frequencyDataIntervalId = (0, import_react2.useRef)(null);
  const onPlayAudio = (0, import_react2.useRef)(props.onPlayAudio);
  onPlayAudio.current = props.onPlayAudio;
  const onStopAudio = (0, import_react2.useRef)(props.onStopAudio);
  onStopAudio.current = props.onStopAudio;
  const onError = (0, import_react2.useRef)(props.onError);
  onError.current = props.onError;
  const playNextClip = (0, import_react2.useCallback)(() => {
    if (analyserNode.current === null || audioContext.current === null) {
      onError.current("Audio environment is not initialized");
      return;
    }
    if (clipQueue.current.length === 0 || isProcessing.current) {
      setQueueLength(0);
      return;
    }
    const nextClip = clipQueue.current.shift();
    setQueueLength(clipQueue.current.length);
    if (!nextClip) return;
    isProcessing.current = true;
    setIsPlaying(true);
    const bufferSource = audioContext.current.createBufferSource();
    bufferSource.buffer = nextClip.buffer;
    bufferSource.connect(analyserNode.current);
    currentlyPlayingAudioBuffer.current = bufferSource;
    const updateFrequencyData = () => {
      try {
        const bufferSampleRate = bufferSource.buffer?.sampleRate;
        if (!analyserNode.current || typeof bufferSampleRate === "undefined")
          return;
        const dataArray = new Uint8Array(
          analyserNode.current.frequencyBinCount
        );
        analyserNode.current.getByteFrequencyData(dataArray);
        const barkFrequencies = convertLinearFrequenciesToBark(
          dataArray,
          bufferSampleRate
        );
        setFft(() => barkFrequencies);
      } catch (e) {
        setFft(generateEmptyFft());
      }
    };
    frequencyDataIntervalId.current = window.setInterval(
      updateFrequencyData,
      5
    );
    bufferSource.start(0);
    onPlayAudio.current(nextClip.id);
    bufferSource.onended = () => {
      if (frequencyDataIntervalId.current) {
        clearInterval(frequencyDataIntervalId.current);
      }
      setFft(generateEmptyFft());
      bufferSource.disconnect();
      isProcessing.current = false;
      setIsPlaying(false);
      onStopAudio.current(nextClip.id);
      currentlyPlayingAudioBuffer.current = null;
      playNextClip();
    };
  }, []);
  const initPlayer = (0, import_react2.useCallback)(() => {
    const initAudioContext = new AudioContext();
    audioContext.current = initAudioContext;
    const analyser = initAudioContext.createAnalyser();
    const gain = initAudioContext.createGain();
    analyser.fftSize = 2048;
    analyser.connect(gain);
    gain.connect(initAudioContext.destination);
    analyserNode.current = analyser;
    gainNode.current = gain;
    isInitialized.current = true;
  }, []);
  const addToQueue = (0, import_react2.useCallback)(
    async (message) => {
      if (!isInitialized.current || !audioContext.current) {
        onError.current("Audio player has not been initialized");
        return;
      }
      try {
        const blob = (0, import_hume2.convertBase64ToBlob)(message.data, "audio/mp3");
        const arrayBuffer = await blob.arrayBuffer();
        const audioBuffer = await audioContext.current.decodeAudioData(arrayBuffer);
        clipQueue.current.push({
          id: message.id,
          buffer: audioBuffer
        });
        setQueueLength(clipQueue.current.length);
        if (clipQueue.current.length === 1) {
          playNextClip();
        }
      } catch (e) {
        const eMessage = e instanceof Error ? e.message : "Unknown error";
        onError.current(`Failed to add clip to queue: ${eMessage}`);
      }
    },
    [playNextClip]
  );
  const stopAll = (0, import_react2.useCallback)(() => {
    isInitialized.current = false;
    isProcessing.current = false;
    setIsPlaying(false);
    setIsAudioMuted(false);
    if (frequencyDataIntervalId.current) {
      window.clearInterval(frequencyDataIntervalId.current);
    }
    if (currentlyPlayingAudioBuffer.current) {
      currentlyPlayingAudioBuffer.current.disconnect();
      currentlyPlayingAudioBuffer.current = null;
    }
    if (analyserNode.current) {
      analyserNode.current.disconnect();
      analyserNode.current = null;
    }
    if (audioContext.current) {
      void audioContext.current.close().then(() => {
        audioContext.current = null;
      }).catch(() => {
        return null;
      });
    }
    clipQueue.current = [];
    setQueueLength(0);
    setFft(generateEmptyFft());
  }, []);
  const clearQueue = (0, import_react2.useCallback)(() => {
    if (currentlyPlayingAudioBuffer.current) {
      currentlyPlayingAudioBuffer.current.stop();
      currentlyPlayingAudioBuffer.current = null;
    }
    clipQueue.current = [];
    setQueueLength(0);
    isProcessing.current = false;
    setIsPlaying(false);
    setFft(generateEmptyFft());
  }, []);
  const muteAudio = (0, import_react2.useCallback)(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(0, audioContext.current.currentTime);
      setIsAudioMuted(true);
    }
  }, []);
  const unmuteAudio = (0, import_react2.useCallback)(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(1, audioContext.current.currentTime);
      setIsAudioMuted(false);
    }
  }, []);
  return {
    addToQueue,
    fft,
    initPlayer,
    isPlaying,
    isAudioMuted,
    muteAudio,
    unmuteAudio,
    stopAll,
    clearQueue,
    queueLength
  };
};

// src/lib/useVoiceClient.ts
var import_hume3 = require("hume");
var import_react3 = require("react");
var isNever = (_n) => {
  return;
};
var VoiceReadyState = /* @__PURE__ */ ((VoiceReadyState2) => {
  VoiceReadyState2["IDLE"] = "idle";
  VoiceReadyState2["CONNECTING"] = "connecting";
  VoiceReadyState2["OPEN"] = "open";
  VoiceReadyState2["CLOSED"] = "closed";
  return VoiceReadyState2;
})(VoiceReadyState || {});
var useVoiceClient = (props) => {
  const client = (0, import_react3.useRef)(null);
  const [readyState, setReadyState] = (0, import_react3.useState)(
    "idle" /* IDLE */
  );
  const onAudioMessage = (0, import_react3.useRef)(
    props.onAudioMessage
  );
  onAudioMessage.current = props.onAudioMessage;
  const onMessage = (0, import_react3.useRef)(props.onMessage);
  onMessage.current = props.onMessage;
  const onToolCall = (0, import_react3.useRef)(props.onToolCall);
  onToolCall.current = props.onToolCall;
  const onError = (0, import_react3.useRef)(props.onError);
  onError.current = props.onError;
  const onOpen = (0, import_react3.useRef)(props.onOpen);
  onOpen.current = props.onOpen;
  const onClose = (0, import_react3.useRef)(props.onClose);
  onClose.current = props.onClose;
  const connect = (0, import_react3.useCallback)((config) => {
    return new Promise((resolve, reject) => {
      const hume = new import_hume3.HumeClient(
        config.auth.type === "apiKey" ? {
          apiKey: config.auth.value,
          environment: config.hostname
        } : {
          accessToken: config.auth.value,
          environment: config.hostname
        }
      );
      client.current = hume.empathicVoice.chat.connect(config);
      client.current.on("open", () => {
        onOpen.current?.();
        setReadyState("open" /* OPEN */);
        resolve("open" /* OPEN */);
      });
      client.current.on("message", (message) => {
        if (message.type === "audio_output") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onAudioMessage.current?.(messageWithReceivedAt);
          return;
        }
        if (message.type === "assistant_message" || message.type === "user_message" || message.type === "user_interruption" || message.type === "error" || message.type === "tool_response" || message.type === "tool_error" || message.type === "chat_metadata" || message.type === "assistant_end") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onMessage.current?.(messageWithReceivedAt);
          return;
        }
        if (message.type === "tool_call") {
          const messageWithReceivedAt = { ...message, receivedAt: /* @__PURE__ */ new Date() };
          onMessage.current?.(messageWithReceivedAt);
          if (message.toolType === import_hume3.Hume.empathicVoice.ToolType.Function) {
            void onToolCall.current?.(
              {
                ...messageWithReceivedAt,
                // we have to do this because even though we are using the correct
                // enum on line 30 for the type definition
                // fern exports an interface and a value using the same `ToolType`
                // identifier so the type comparisons will always fail
                toolType: "function"
              },
              {
                success: (content) => ({
                  type: "tool_response",
                  toolCallId: messageWithReceivedAt.toolCallId,
                  content: JSON.stringify(content)
                }),
                error: ({
                  error,
                  code,
                  level,
                  content
                }) => ({
                  type: "tool_error",
                  toolCallId: messageWithReceivedAt.toolCallId,
                  error,
                  code,
                  level: level !== null ? "warn" : void 0,
                  // level can only be warn
                  content
                })
              }
            ).then((response) => {
              if (response.type === "tool_response") {
                client.current?.sendToolResponseMessage(response);
              } else if (response.type === "tool_error") {
                client.current?.sendToolErrorMessage(response);
              } else {
                onError.current?.("Invalid response from tool call");
              }
            });
          }
          return;
        }
        isNever(message);
        return;
      });
      client.current.on("close", (event) => {
        onClose.current?.(event);
        setReadyState("closed" /* CLOSED */);
      });
      client.current.on("error", (e) => {
        const message = e instanceof Error ? e.message : "Unknown error";
        onError.current?.(message, e instanceof Error ? e : void 0);
        reject(e);
      });
      setReadyState("connecting" /* CONNECTING */);
    });
  }, []);
  const disconnect = (0, import_react3.useCallback)(() => {
    setReadyState("idle" /* IDLE */);
    client.current?.close();
  }, []);
  const sendSessionSettings = (0, import_react3.useCallback)(
    (sessionSettings) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendSessionSettings(sessionSettings);
    },
    [readyState]
  );
  const sendAudio = (0, import_react3.useCallback)(
    (arrayBuffer) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.socket?.send(arrayBuffer);
    },
    [readyState]
  );
  const sendUserInput = (0, import_react3.useCallback)(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendUserInput(text);
    },
    [readyState]
  );
  const sendAssistantInput = (0, import_react3.useCallback)(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      client.current?.sendAssistantInput({
        text
      });
    },
    [readyState]
  );
  const sendToolMessage = (0, import_react3.useCallback)(
    (toolMessage) => {
      if (readyState !== "open" /* OPEN */) {
        throw new Error("Socket is not open");
      }
      if (toolMessage.type === "tool_error") {
        client.current?.sendToolErrorMessage(toolMessage);
      } else {
        client.current?.sendToolResponseMessage(toolMessage);
      }
    },
    [readyState]
  );
  const sendPauseAssistantMessage = (0, import_react3.useCallback)(() => {
    if (readyState !== "open" /* OPEN */) {
      throw new Error("Socket is not open");
    }
    client.current?.pauseAssistant({});
  }, [readyState]);
  const sendResumeAssistantMessage = (0, import_react3.useCallback)(() => {
    if (readyState !== "open" /* OPEN */) {
      throw new Error("Socket is not open");
    }
    client.current?.resumeAssistant({});
  }, [readyState]);
  return {
    readyState,
    sendSessionSettings,
    sendAudio,
    connect,
    disconnect,
    sendUserInput,
    sendAssistantInput,
    sendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  };
};

// src/lib/VoiceProvider.tsx
var import_react8 = require("react");

// src/lib/noop.ts
var noop = () => {
};

// src/lib/useCallDuration.ts
var import_date_fns = require("date-fns");
var import_react4 = require("react");
var useCallDuration = () => {
  const interval = (0, import_react4.useRef)(null);
  const startTime = (0, import_react4.useRef)(null);
  const [timestamp, setTimestamp] = (0, import_react4.useState)(null);
  const start = (0, import_react4.useCallback)(() => {
    startTime.current = Date.now();
    setTimestamp("00:00:00");
    interval.current = window.setInterval(() => {
      if (startTime.current) {
        const duration = (0, import_date_fns.intervalToDuration)({
          start: startTime.current,
          end: Date.now()
        });
        const hours = (duration.hours ?? 0).toString().padStart(2, "0");
        const minutes = (duration.minutes ?? 0).toString().padStart(2, "0");
        const seconds = (duration.seconds ?? 0).toString().padStart(2, "0");
        setTimestamp(`${hours}:${minutes}:${seconds}`);
      }
    }, 500);
  }, []);
  const stop = (0, import_react4.useCallback)(() => {
    if (interval.current) {
      window.clearInterval(interval.current);
      interval.current = null;
    }
  }, []);
  const reset = (0, import_react4.useCallback)(() => {
    setTimestamp(null);
  }, []);
  (0, import_react4.useEffect)(() => {
    return () => {
      if (interval.current) {
        window.clearInterval(interval.current);
        interval.current = null;
      }
    };
  }, []);
  return { timestamp, start, stop, reset };
};

// src/lib/useEncoding.ts
var import_hume4 = require("hume");
var import_react5 = require("react");
var useEncoding = () => {
  const [permission, setPermission] = (0, import_react5.useState)("prompt");
  const streamRef = (0, import_react5.useRef)(null);
  const getStream = (0, import_react5.useCallback)(async () => {
    try {
      const stream = await (0, import_hume4.getAudioStream)();
      setPermission("granted");
      streamRef.current = stream;
      (0, import_hume4.checkForAudioTracks)(stream);
      return "granted";
    } catch (e) {
      setPermission("denied");
      return "denied";
    }
  }, []);
  return {
    streamRef,
    getStream,
    permission
  };
};

// src/lib/useMessages.ts
var import_react6 = require("react");

// src/utils/index.ts
var keepLastN = (n, arr) => {
  if (arr.length <= n) {
    return arr;
  }
  return arr.slice(arr.length - n);
};

// src/lib/useMessages.ts
var useMessages = ({
  sendMessageToParent,
  messageHistoryLimit
}) => {
  const [voiceMessageMap, setVoiceMessageMap] = (0, import_react6.useState)({});
  const [messages, setMessages] = (0, import_react6.useState)([]);
  const [lastVoiceMessage, setLastVoiceMessage] = (0, import_react6.useState)(null);
  const [lastUserMessage, setLastUserMessage] = (0, import_react6.useState)(null);
  const [chatMetadata, setChatMetadata] = (0, import_react6.useState)(
    null
  );
  const createConnectMessage = (0, import_react6.useCallback)(() => {
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_connected",
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const createDisconnectMessage = (0, import_react6.useCallback)((event) => {
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_disconnected",
          code: event.code,
          reason: event.reason,
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const onMessage = (0, import_react6.useCallback)(
    (message) => {
      switch (message.type) {
        case "assistant_message":
          setVoiceMessageMap((prev) => ({
            ...prev,
            [`${message.id}`]: message
          }));
          break;
        case "user_message":
          sendMessageToParent?.(message);
          if (message.interim === false) {
            setLastUserMessage(message);
            setMessages((prev) => {
              return keepLastN(messageHistoryLimit, prev.concat([message]));
            });
          }
          break;
        case "user_interruption":
        case "error":
        case "tool_call":
        case "tool_response":
        case "tool_error":
        case "assistant_end":
          sendMessageToParent?.(message);
          setMessages((prev) => {
            return keepLastN(messageHistoryLimit, prev.concat([message]));
          });
          break;
        case "chat_metadata":
          sendMessageToParent?.(message);
          setMessages((prev) => {
            return keepLastN(messageHistoryLimit, prev.concat([message]));
          });
          setChatMetadata(message);
          break;
        default:
          break;
      }
    },
    [messageHistoryLimit, sendMessageToParent]
  );
  const onPlayAudio = (0, import_react6.useCallback)(
    (id) => {
      const matchingTranscript = voiceMessageMap[id];
      if (matchingTranscript) {
        sendMessageToParent?.(matchingTranscript);
        setLastVoiceMessage(matchingTranscript);
        setMessages((prev) => {
          return keepLastN(
            messageHistoryLimit,
            prev.concat([matchingTranscript])
          );
        });
        setVoiceMessageMap((prev) => {
          const newMap = { ...prev };
          delete newMap[id];
          return newMap;
        });
      }
    },
    [voiceMessageMap, sendMessageToParent, messageHistoryLimit]
  );
  const clearMessages = (0, import_react6.useCallback)(() => {
    setMessages([]);
    setLastVoiceMessage(null);
    setLastUserMessage(null);
    setVoiceMessageMap({});
  }, []);
  return {
    createConnectMessage,
    createDisconnectMessage,
    onMessage,
    onPlayAudio,
    clearMessages,
    messages,
    lastVoiceMessage,
    lastUserMessage,
    chatMetadata
  };
};

// src/lib/useToolStatus.ts
var import_react7 = require("react");
var useToolStatus = () => {
  const [store, setStore] = (0, import_react7.useState)({});
  const addToStore = (0, import_react7.useCallback)(
    (message) => {
      setStore((prev) => {
        const entry = {
          ...prev[message.toolCallId]
        };
        if (message.type === "tool_call") {
          entry.call = message;
        }
        if (message.type === "tool_response" || message.type === "tool_error") {
          entry.resolved = message;
        }
        return {
          ...prev,
          [message.toolCallId]: entry
        };
      });
    },
    []
  );
  const clearStore = (0, import_react7.useCallback)(() => {
    setStore({});
  }, []);
  return {
    store,
    addToStore,
    clearStore
  };
};

// src/lib/VoiceProvider.tsx
var import_jsx_runtime = require("react/jsx-runtime");
var VoiceContext = (0, import_react8.createContext)(null);
var useVoice = () => {
  const ctx = (0, import_react8.useContext)(VoiceContext);
  if (!ctx) {
    throw new Error("useVoice must be used within an VoiceProvider");
  }
  return ctx;
};
var VoiceProvider = ({
  children,
  clearMessagesOnDisconnect = true,
  messageHistoryLimit = 100,
  sessionSettings,
  verboseTranscription = true,
  ...props
}) => {
  const {
    timestamp: callDurationTimestamp,
    start: startTimer,
    stop: stopTimer
  } = useCallDuration();
  const [status, setStatus] = (0, import_react8.useState)({
    value: "disconnected"
  });
  const [isPaused, setIsPaused] = (0, import_react8.useState)(false);
  const [error, setError] = (0, import_react8.useState)(null);
  const isError = error !== null;
  const isMicrophoneError = error?.type === "mic_error";
  const isSocketError = error?.type === "socket_error";
  const isAudioError = error?.type === "audio_error";
  const onError = (0, import_react8.useRef)(props.onError ?? noop);
  onError.current = props.onError ?? noop;
  const onClose = (0, import_react8.useRef)(props.onClose ?? noop);
  onClose.current = props.onClose ?? noop;
  const onMessage = (0, import_react8.useRef)(props.onMessage ?? noop);
  onMessage.current = props.onMessage ?? noop;
  const onAudioReceived = (0, import_react8.useRef)(props.onAudioReceived ?? noop);
  onAudioReceived.current = props.onAudioReceived ?? noop;
  const onAudioStart = (0, import_react8.useRef)(props.onAudioStart ?? noop);
  onAudioStart.current = props.onAudioStart ?? noop;
  const onAudioEnd = (0, import_react8.useRef)(props.onAudioEnd ?? noop);
  onAudioEnd.current = props.onAudioEnd ?? noop;
  const onInterruption = (0, import_react8.useRef)(props.onInterruption ?? noop);
  onInterruption.current = props.onInterruption ?? noop;
  const toolStatus = useToolStatus();
  const messageStore = useMessages({
    sendMessageToParent: onMessage.current,
    messageHistoryLimit
  });
  const updateError = (0, import_react8.useCallback)((err) => {
    setError(err);
    if (err !== null) {
      onError.current?.(err);
    }
  }, []);
  const onClientError = (0, import_react8.useCallback)(
    (message, err) => {
      stopTimer();
      updateError({ type: "socket_error", message, error: err });
    },
    [stopTimer, updateError]
  );
  const config = props;
  const player = useSoundPlayer({
    onError: (message) => {
      updateError({ type: "audio_error", message });
    },
    onPlayAudio: (id) => {
      messageStore.onPlayAudio(id);
      onAudioStart.current(id);
    },
    onStopAudio: (id) => {
      onAudioEnd.current(id);
    }
  });
  const { streamRef, getStream, permission: micPermission } = useEncoding();
  const client = useVoiceClient({
    onAudioMessage: (message) => {
      player.addToQueue(message);
      onAudioReceived.current(message);
    },
    onMessage: (0, import_react8.useCallback)(
      (message) => {
        messageStore.onMessage(message);
        if (message.type === "user_interruption" || message.type === "user_message") {
          if (player.isPlaying) {
            onInterruption.current(message);
          }
          player.clearQueue();
        }
        if (message.type === "tool_call" || message.type === "tool_response" || message.type === "tool_error") {
          toolStatus.addToStore(message);
        }
      },
      [messageStore, player, toolStatus]
    ),
    onError: onClientError,
    onOpen: (0, import_react8.useCallback)(() => {
      startTimer();
      messageStore.createConnectMessage();
      props.onOpen?.();
    }, [messageStore, props, startTimer]),
    onClose: (0, import_react8.useCallback)(
      (event) => {
        stopTimer();
        messageStore.createDisconnectMessage(event);
        onClose.current?.(event);
      },
      [messageStore, stopTimer]
    ),
    onToolCall: props.onToolCall
  });
  const {
    sendAudio: clientSendAudio,
    sendUserInput: clientSendUserInput,
    sendAssistantInput: clientSendAssistantInput,
    sendSessionSettings: clientSendSessionSettings,
    sendToolMessage: clientSendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  } = client;
  const mic = useMicrophone({
    streamRef,
    onAudioCaptured: (0, import_react8.useCallback)(
      (arrayBuffer) => {
        try {
          clientSendAudio(arrayBuffer);
        } catch (e) {
          const message = e instanceof Error ? e.message : "Unknown error";
          updateError({ type: "socket_error", message });
        }
      },
      [clientSendAudio, updateError]
    ),
    onError: (0, import_react8.useCallback)(
      (message) => {
        updateError({ type: "mic_error", message });
      },
      [updateError]
    )
  });
  const { clearQueue } = player;
  const pauseAssistant = (0, import_react8.useCallback)(() => {
    try {
      sendPauseAssistantMessage();
      setIsPaused(true);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({ type: "socket_error", message });
    }
    clearQueue();
  }, [sendPauseAssistantMessage, clearQueue, updateError]);
  const resumeAssistant = (0, import_react8.useCallback)(() => {
    try {
      sendResumeAssistantMessage();
      setIsPaused(false);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({ type: "socket_error", message });
    }
  }, [sendResumeAssistantMessage, updateError]);
  const connect = (0, import_react8.useCallback)(async () => {
    updateError(null);
    setStatus({ value: "connecting" });
    const permission = await getStream();
    if (permission === "denied") {
      const error2 = {
        type: "mic_error",
        message: "Microphone permission denied"
      };
      updateError(error2);
      return Promise.reject(error2);
    }
    try {
      await client.connect({
        ...config,
        verboseTranscription: true
      });
    } catch (e) {
      const error2 = {
        type: "socket_error",
        message: "We could not connect to the voice. Please try again."
      };
      updateError(error2);
      return Promise.reject(error2);
    }
    try {
      const [micPromise, playerPromise] = await Promise.allSettled([
        mic.start(),
        player.initPlayer()
      ]);
      if (micPromise.status === "fulfilled" && playerPromise.status === "fulfilled") {
        setStatus({ value: "connected" });
      }
    } catch (e) {
      const error2 = {
        type: "audio_error",
        message: e instanceof Error ? e.message : "We could not connect to audio. Please try again."
      };
      updateError(error2);
    }
  }, [client, config, getStream, mic, player, updateError]);
  const disconnectFromVoice = (0, import_react8.useCallback)(() => {
    if (client.readyState !== "closed" /* CLOSED */) {
      client.disconnect();
    }
    player.stopAll();
    mic.stop();
    if (clearMessagesOnDisconnect) {
      messageStore.clearMessages();
    }
    toolStatus.clearStore();
    setIsPaused(false);
  }, [
    client,
    player,
    mic,
    clearMessagesOnDisconnect,
    toolStatus,
    messageStore
  ]);
  const disconnect = (0, import_react8.useCallback)(
    (disconnectOnError) => {
      if (micPermission === "denied") {
        setStatus({ value: "error", reason: "Microphone permission denied" });
      }
      stopTimer();
      disconnectFromVoice();
      if (status.value !== "error" && !disconnectOnError) {
        setStatus({ value: "disconnected" });
      }
    },
    [micPermission, stopTimer, disconnectFromVoice, status.value]
  );
  (0, import_react8.useEffect)(() => {
    if (error !== null && status.value !== "error" && status.value !== "disconnected") {
      setStatus({ value: "error", reason: error.message });
      disconnectFromVoice();
    }
  }, [status.value, disconnect, disconnectFromVoice, error]);
  (0, import_react8.useEffect)(() => {
    return () => {
      disconnectFromVoice();
    };
  }, []);
  const sendUserInput = (0, import_react8.useCallback)(
    (text) => {
      try {
        clientSendUserInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendUserInput, updateError]
  );
  const sendAssistantInput = (0, import_react8.useCallback)(
    (text) => {
      try {
        clientSendAssistantInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendAssistantInput, updateError]
  );
  const sendSessionSettings = (0, import_react8.useCallback)(
    (sessionSettings2) => {
      try {
        clientSendSessionSettings(sessionSettings2);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message });
      }
    },
    [clientSendSessionSettings, updateError]
  );
  (0, import_react8.useEffect)(() => {
    if (client.readyState === "open" /* OPEN */ && sessionSettings !== void 0 && Object.keys(sessionSettings).length > 0) {
      sendSessionSettings(sessionSettings);
    }
  }, [client.readyState, sendSessionSettings, sessionSettings]);
  const sendToolMessage = (0, import_react8.useCallback)(
    (message) => {
      try {
        clientSendToolMessage(message);
      } catch (e) {
        const message2 = e instanceof Error ? e.message : "Unknown error";
        updateError({ type: "socket_error", message: message2 });
      }
    },
    [clientSendToolMessage, updateError]
  );
  const ctx = (0, import_react8.useMemo)(
    () => ({
      connect,
      disconnect,
      fft: player.fft,
      micFft: mic.fft,
      isMuted: mic.isMuted,
      isAudioMuted: player.isAudioMuted,
      isPlaying: player.isPlaying,
      messages: messageStore.messages,
      lastVoiceMessage: messageStore.lastVoiceMessage,
      lastUserMessage: messageStore.lastUserMessage,
      clearMessages: messageStore.clearMessages,
      mute: mic.mute,
      muteAudio: player.muteAudio,
      readyState: client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      unmute: mic.unmute,
      unmuteAudio: player.unmuteAudio,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatusStore: toolStatus.store,
      chatMetadata: messageStore.chatMetadata,
      playerQueueLength: player.queueLength,
      isPaused
    }),
    [
      connect,
      disconnect,
      player.fft,
      player.isAudioMuted,
      player.isPlaying,
      player.muteAudio,
      player.unmuteAudio,
      player.queueLength,
      mic.fft,
      mic.isMuted,
      mic.mute,
      mic.unmute,
      messageStore.messages,
      messageStore.lastVoiceMessage,
      messageStore.lastUserMessage,
      messageStore.clearMessages,
      messageStore.chatMetadata,
      client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatus.store,
      isPaused
    ]
  );
  return /* @__PURE__ */ (0, import_jsx_runtime.jsx)(VoiceContext.Provider, { value: ctx, children });
};

// src/lib/errors.ts
var SocketUnknownMessageError = class extends Error {
  constructor(message) {
    super(`Unknown message type.${message ? " " + message : ""}`);
    this.name = "SocketUnknownMessageError";
  }
};
var isSocketUnknownMessageError = (err) => {
  return err instanceof SocketUnknownMessageError;
};
var SocketFailedToParseMessageError = class extends Error {
  constructor(message) {
    super(
      `Failed to parse message from socket.${message ? " " + message : ""}`
    );
    this.name = "SocketFailedToParseMessageError";
  }
};
var isSocketFailedToParseMessageError = (err) => {
  return err instanceof SocketFailedToParseMessageError;
};

// src/lib/messages.ts
var import_empathicVoice = require("hume/serialization/resources/empathicVoice/index.js");

// src/lib/audio-message.ts
var import_zod = __toESM(require("zod"));
var AudioMessageSchema = import_zod.default.object({
  type: import_zod.default.literal("audio"),
  data: import_zod.default.instanceof(ArrayBuffer)
}).transform((obj) => {
  return Object.assign(obj, {
    receivedAt: /* @__PURE__ */ new Date()
  });
});
var parseAudioMessage = async (blob) => {
  return blob.arrayBuffer().then((buffer) => {
    return {
      type: "audio",
      data: buffer,
      receivedAt: /* @__PURE__ */ new Date()
    };
  }).catch(() => {
    return null;
  });
};

// src/lib/messages.ts
var parseMessageData = async (data) => {
  if (data instanceof Blob) {
    const message = await parseAudioMessage(data);
    if (message) {
      return {
        success: true,
        message
      };
    } else {
      return {
        success: false,
        error: new SocketFailedToParseMessageError(
          `Received blob was unable to be converted to ArrayBuffer.`
        )
      };
    }
  }
  if (typeof data !== "string") {
    return {
      success: false,
      error: new SocketFailedToParseMessageError(
        `Expected a string but received ${typeof data}.`
      )
    };
  }
  const parseResponse = import_empathicVoice.SubscribeEvent.parse(data);
  if (!parseResponse.ok) {
    return {
      success: false,
      error: new SocketUnknownMessageError(
        `Received JSON was not a known message type.`
      )
    };
  }
  return {
    success: true,
    message: parseResponse.value
  };
};
var parseMessageType = async (event) => {
  const data = event.data;
  return parseMessageData(data);
};

// src/models/audio.ts
var Channels = /* @__PURE__ */ ((Channels2) => {
  Channels2[Channels2["MONO"] = 1] = "MONO";
  Channels2[Channels2["STEREO"] = 2] = "STEREO";
  return Channels2;
})(Channels || {});
var AudioEncoding = /* @__PURE__ */ ((AudioEncoding2) => {
  AudioEncoding2["LINEAR16"] = "linear16";
  AudioEncoding2["OPUS"] = "opus";
  return AudioEncoding2;
})(AudioEncoding || {});

// src/models/llm.ts
var LanguageModelOption = /* @__PURE__ */ ((LanguageModelOption2) => {
  LanguageModelOption2["CLAUDE_3_OPUS"] = "CLAUDE_3_OPUS";
  LanguageModelOption2["CLAUDE_3_SONNET"] = "CLAUDE_3_SONNET";
  LanguageModelOption2["CLAUDE_3_HAIKU"] = "CLAUDE_3_HAIKU";
  LanguageModelOption2["CLAUDE_21"] = "CLAUDE_21";
  LanguageModelOption2["CLAUDE_INSTANT_12"] = "CLAUDE_INSTANT_12";
  LanguageModelOption2["GPT_4_TURBO_PREVIEW"] = "GPT_4_TURBO_PREVIEW";
  LanguageModelOption2["GPT_35_TURBO_0125"] = "GPT_35_TURBO_0125";
  LanguageModelOption2["GPT_35_TURBO"] = "GPT_35_TURBO";
  LanguageModelOption2["FIREWORKS_MIXTRAL_8X7B"] = "FIREWORKS_MIXTRAL_8X7B";
  return LanguageModelOption2;
})(LanguageModelOption || {});

// src/models/messages.ts
var import_zod2 = __toESM(require("zod"));
var TimeSliceSchema = import_zod2.default.object({
  begin: import_zod2.default.number(),
  end: import_zod2.default.number()
});

// src/models/ttsService.ts
var TTSService = /* @__PURE__ */ ((TTSService2) => {
  TTSService2["DEFAULT"] = "hume_ai";
  TTSService2["ELEVEN_LABS"] = "eleven_labs";
  TTSService2["PLAY_HT"] = "play_ht";
  return TTSService2;
})(TTSService || {});
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  AudioEncoding,
  Channels,
  LanguageModelOption,
  SocketFailedToParseMessageError,
  SocketUnknownMessageError,
  TTSService,
  TimeSliceSchema,
  VoiceProvider,
  VoiceReadyState,
  isSocketFailedToParseMessageError,
  isSocketUnknownMessageError,
  parseMessageData,
  parseMessageType,
  useMicrophone,
  useSoundPlayer,
  useVoice,
  useVoiceClient
});
//# sourceMappingURL=index.js.map